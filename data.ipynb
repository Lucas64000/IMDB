{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d97212-7453-4599-b786-ba18af2f9c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cytech/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cytech/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "ds = datasets.load_from_disk(\"data/imdb_dataset/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c33c33d2-7eb5-446c-a698-1214ea1ba40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds['train']\n",
    "val_ds = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1f7dcf-c258-41b8-82ea-b5ea7deaaccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "class TextProcessing:\n",
    "    def __init__(self, lemmatizer=WordNetLemmatizer(), stop_words=None):\n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = text.lower()\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        if self.stop_words is not None:\n",
    "            tokens = [token for token in tokens if not token in self.stop_words]\n",
    "        \n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return lemmatized_tokens     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8456b642-312b-45d3-9ab8-a4a289bf87ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cytech/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, processing=AutoTokenizer.from_pretrained('bert-base-uncased'), min_freq=10):\n",
    "        self.processing = processing\n",
    "        self.min_freq = min_freq\n",
    "        #self.max_words = max_words\n",
    "        self.idx_to_str = {}\n",
    "        self.str_to_idx = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_sentence(sentence):\n",
    "        return re.sub(r'[^a-zA-Z\\s]', ' ', re.sub(r'[.-]', ' ', re.sub(r'<.*?>', '', sentence))).strip()\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter() \n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            sentence = self.clean_sentence(sentence)\n",
    "            words = self.processing.tokenize(sentence)  \n",
    "            frequencies.update(words) \n",
    "\n",
    "        frequencies = {k: v for k, v in frequencies.items() if v > self.min_freq}\n",
    "        frequencies = dict(sorted(frequencies.items(), key=lambda x: -x[1]))\n",
    "\n",
    "        for idx, word in enumerate(frequencies.keys(), start=len(self.idx_to_str)):\n",
    "            self.str_to_idx[word] = idx\n",
    "            self.idx_to_str[idx] = word\n",
    "\n",
    "    def numericalize(self, sentence):\n",
    "        sentence = self.clean_sentence(sentence)\n",
    "        tokens = self.processing.encode(sentence, truncation=True, max_length=512, padding='max_length')\n",
    "        \n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8c8cc0f-39e8-4cda-97c0-11e69e5684ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImdbDataset(Dataset):\n",
    "    def __init__(self, df, max_length=512, min_freq=2):\n",
    "        self.df = df\n",
    "        self.reviews = self.df['text']\n",
    "        self.labels = self.df['label']\n",
    "\n",
    "        self.vocab = Vocabulary()\n",
    "        self.vocab.build_vocabulary(self.reviews)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        review = self.reviews[idx]\n",
    "\n",
    "        tokenized_review = self.vocab.numericalize(review) \n",
    "        \n",
    "        return torch.tensor(tokenized_review), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c24bfbc9-8363-410e-a667-ababbec25c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_ds = CustomImdbDataset(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01c68cf6-f3e1-4490-9801-df112199c168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  1045, 12524,  1045,  2572,  8025,  3756,  2013,  2026,  2678,\n",
       "          3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,  2043,\n",
       "          2009,  2001,  2034,  2207,  1999,  1045,  2036,  2657,  2008,  2012,\n",
       "          2034,  2009,  2001,  8243,  2011,  1057,  1055,  8205,  2065,  2009,\n",
       "          2412,  2699,  2000,  4607,  2023,  2406,  3568,  2108,  1037,  5470,\n",
       "          1997,  3152,  2641,  6801,  1045,  2428,  2018,  2000,  2156,  2023,\n",
       "          2005,  2870,  1996,  5436,  2003,  8857,  2105,  1037,  2402,  4467,\n",
       "          3689,  3076,  2315, 14229,  2040,  4122,  2000,  4553,  2673,  2016,\n",
       "          2064,  2055,  2166,  1999,  3327,  2016,  4122,  2000,  3579,  2014,\n",
       "          3086,  2015,  2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,\n",
       "          1996,  2779, 25430, 14728,  2245,  2055,  3056,  2576,  3314,  2107,\n",
       "          2004,  1996,  5148,  2162,  1998,  2679,  3314,  1999,  1996,  2142,\n",
       "          2163,  1999,  2090,  4851,  8801,  1998,  6623,  7939,  4697,  3619,\n",
       "          1997,  8947,  2055,  2037, 10740,  2006,  4331,  2016,  2038,  3348,\n",
       "          2007,  2014,  3689,  3836, 19846,  1998,  2496,  2273,  2054,  8563,\n",
       "          2033,  2055,  1045,  2572,  8025,  3756,  2003,  2008,  2086,  3283,\n",
       "          2023,  2001,  2641, 26932,  2428,  1996,  3348,  1998, 16371, 25469,\n",
       "          5019,  2024,  2261,  1998,  2521,  2090,  2130,  2059,  2009,  1055,\n",
       "          2025,  2915,  2066,  2070, 10036,  2135,  2081, 22555,  2080,  2096,\n",
       "          2026,  2406,  3549,  2568,  2424,  2009, 16880,  1999,  4507,  3348,\n",
       "          1998, 16371, 25469,  2024,  1037,  2350, 18785,  1999,  4467,  5988,\n",
       "          2130, 13749,  7849, 24544, 15835,  2037,  3437,  2000,  2204,  2214,\n",
       "          2879,  2198,  4811,  2018,  3348,  5019,  1999,  2010,  3152,  1045,\n",
       "          2079,  4012,  3549,  2094,  1996, 16587,  2005,  1996,  2755,  2008,\n",
       "          2151,  3348,  3491,  1999,  1996,  2143,  2003,  3491,  2005,  6018,\n",
       "          5682,  2738,  2084,  2074,  2000,  5213,  2111,  1998,  2191,  2769,\n",
       "          2000,  2022,  3491,  1999, 26932, 12370,  1999,  2637,  1045,  2572,\n",
       "          8025,  3756,  2003,  1037,  2204,  2143,  2005,  3087,  5782,  2000,\n",
       "          2817,  1996,  6240,  1998, 14629,  2053, 26136,  3832,  1997,  4467,\n",
       "          5988,  2021,  2428,  2023,  2143,  2987,  1056,  2031,  2172,  1997,\n",
       "          1037,  5436,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review, label = train_ds[0]\n",
    "review, label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
